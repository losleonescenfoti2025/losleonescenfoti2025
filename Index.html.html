<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arquitectura Transformer - Todo lo que Necesitas Saber</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Montserrat:wght@500;700&display=swap" rel="stylesheet">
    <!-- MathJax for LaTeX rendering -->
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        :root {
            --primary-color: #0f3460;
            --secondary-color: #16213e;
            --accent-color: #e94560;
            --background-light: #f9f9f9;
            --background-white: #ffffff;
            --text-dark: #333;
            --text-light: #fff;
            --shadow-light: rgba(0,0,0,0.1);
        }

        body {
            font-family: 'Roboto', Arial, sans-serif;
            background-color: var(--background-light);
            margin: 0;
            padding: 0;
            color: var(--text-dark);
            line-height: 1.8;
        }

        header {
            background-color: var(--secondary-color);
            color: var(--text-light);
            padding: 40px 20px;
            text-align: center;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        header h1 {
            font-family: 'Montserrat', sans-serif;
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }

        nav {
            background-color: var(--primary-color);
            padding: 15px;
            text-align: center;
            box-shadow: 0 2px 5px var(--shadow-light);
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        nav a {
            color: var(--text-light);
            margin: 0 18px;
            text-decoration: none;
            font-weight: 700;
            font-size: 1.05em;
            transition: color 0.3s ease, text-decoration 0.3s ease;
        }

        nav a:hover {
            color: var(--accent-color);
            text-decoration: underline;
        }

        main {
            max-width: 1000px;
            margin: 20px auto;
            padding: 0 20px;
        }

        section {
            padding: 40px 50px;
            background-color: var(--background-white);
            margin-bottom: 20px;
            border-radius: 12px;
            box-shadow: 0 6px 15px var(--shadow-light);
        }

        section img {
            max-width: 100%;
            height: auto;
            margin: 30px auto;
            display: block;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }

        h2 {
            font-family: 'Montserrat', sans-serif;
            color: var(--primary-color);
            margin-bottom: 25px;
            font-size: 2em;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
        }

        h3 {
            font-family: 'Montserrat', sans-serif;
            color: var(--secondary-color);
            margin-top: 35px;
            margin-bottom: 15px;
            font-size: 1.5em;
        }

        h4 { /* Added style for h4 elements */
            font-family: 'Montserrat', sans-serif;
            color: var(--primary-color);
            margin-top: 25px;
            margin-bottom: 10px;
            font-size: 1.3em;
        }

        p {
            margin-bottom: 1em;
        }

        ul {
            list-style-type: none;
            padding: 0;
            margin-bottom: 1em;
        }

        ul li {
            margin-bottom: 12px;
            padding-left: 30px;
            position: relative;
        }

        ul li::before {
            content: '‚ú®'; /* Changed bullet to a more appealing emoji */
            color: var(--primary-color);
            font-weight: bold;
            display: inline-block;
            width: 1em;
            margin-left: -1.5em;
            position: absolute;
            left: 0;
            font-size: 1.1em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 25px;
            margin-bottom: 20px;
            font-size: 0.95em;
        }

        table, th, td {
            border: 1px solid #e0e0e0;
        }

        th, td {
            padding: 15px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            color: var(--primary-color);
            font-family: 'Montserrat', sans-serif;
            font-weight: 700;
        }

        tr:nth-child(even) {
            background-color: #fcfcfc;
        }

        tr:hover {
            background-color: #f5f5f5;
        }

        .equation {
            display: block;
            overflow-x: auto; /* Allows horizontal scrolling for long equations */
            padding: 15px;
            background-color: #eef4f8;
            border-left: 5px solid var(--accent-color);
            margin: 25px 0;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 1.1em;
            color: #333;
        }

        .highlight {
            font-weight: bold;
            color: var(--accent-color);
        }

        .emoji {
            font-size: 1.2em;
            margin-right: 8px;
        }

        footer {
            background-color: var(--secondary-color);
            color: var(--text-light);
            text-align: center;
            padding: 25px;
            margin-top: 40px;
            font-size: 0.9em;
            box-shadow: 0 -2px 8px var(--shadow-light);
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        a:hover {
            color: var(--accent-color);
            text-decoration: underline;
        }

        .video-container {
            position: relative;
            width: 100%;
            padding-bottom: 56.25%; /* 16:9 Aspect Ratio */
            height: 0;
            overflow: hidden;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            margin-top: 30px;
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: 0;
        }
    </style>
</head>
<body>

    <header>
        <h1><span class="emoji">üåê</span> La Arquitectura Transformer: Una Gu√≠a Completa</h1>
        <p>Descubre c√≥mo esta innovaci√≥n ha revolucionado el Procesamiento de Lenguaje Natural y la Inteligencia Artificial.</p>
    </header>

    <nav>
        <a href="#que-es">¬øQu√© es?</a>
        <a href="#componentes">Componentes</a>
        <a href="#flujo">Flujo</a>
        <a href="#ventajas">Ventajas</a>
        <a href="#aplicaciones">Aplicaciones</a>
        <a href="#implementacion">Implementaci√≥n</a>
    </nav>

    <main>
        <section id="que-es">
            <h2><span class="emoji">üß†</span> ¬øQu√© es la Arquitectura Transformer?</h2>
            <p>
                El **Transformer** es una arquitectura de redes neuronales revolucionaria en el campo del Procesamiento de Lenguaje Natural (NLP). Introducida en el paper ‚ÄúAttention is All You Need‚Äù (2017), reemplaza las redes recurrentes (RNN) y las redes de memoria a corto y largo plazo (LSTM) con un nuevo mecanismo: la <span class="highlight">auto-atenci√≥n</span>.
            </p>
            <p>
                Su innovaci√≥n clave radica en eliminar por completo el uso de redes recurrentes (RNN) y convolucionales (CNN), reemplaz√°ndolas con un potente mecanismo de <span class="highlight">atenci√≥n auto-contenido</span> (self-attention).
            </p>
            <p>
                Fue introducido por investigadores de Google Brain y la Universidad de Toronto en la prestigiosa conferencia NIPS 2017, bajo el ic√≥nico lema:
                <br>üëâ <span class="highlight">‚ÄúAttention is all you need‚Äù</span> (La atenci√≥n lo es todo).
            </p>
              <img src="Screenshot-from-2019-06-17-19-53-10.webp" alt="Diagrama de Arquitectura Transformer">
                <figcaption>Representaci√≥n esquem√°tica de la arquitectura Transformer.</figcaption>
            </figure>  

        <section id="componentes">
            <h2><span class="emoji">üß±</span> Componentes Clave de la Arquitectura Transformer</h2>
            <p>
                La red neuronal del Transformer se compone de varias capas modulares que operan en conjunto para procesar secuencias de texto de manera eficiente. A continuaci√≥n, exploramos sus elementos fundamentales:
            </p>

            <h3><span class="emoji">1.</span> Encoder ‚Äì Decoder</h3>
            <ul>
                <li>**Encoder:** Su funci√≥n es <span class="highlight">entender el contexto de la secuencia de entrada</span> y crear una representaci√≥n rica en informaci√≥n. Est√° compuesto por una pila de 6 capas id√©nticas.</li>
                <li>**Decoder:** Su rol es <span class="highlight">generar la secuencia de salida</span> (por ejemplo, la traducci√≥n o la continuaci√≥n de un texto), bas√°ndose en lo que entendi√≥ el encoder. Tambi√©n consta de una pila de 6 capas id√©nticas, similares a las del encoder.</li>
            </ul>
            <figure>
                <img src="Screenshot-from-2019-06-17-20-01-32.webp" alt="Diagrama de Arquitectura Transformer">
                <figcaption>Representaci√≥n esquem√°tica de la arquitectura Transformer, mostrando el encoder y el decoder.</figcaption>
            </figure>  

            <h3><span class="emoji">2.</span> Incrustaciones de Entrada (Embeddings)</h3>
            <p>
                Esta fase inicial es crucial, ya que convierte la informaci√≥n textual de entrada (como una oraci√≥n o un fragmento de c√≥digo) en un formato num√©rico que el modelo puede interpretar y manipular.
            </p>
            <ul>
                <li>Las palabras se convierten en **vectores num√©ricos** (como los generados por Word2Vec, GloVe, ELMo) para que el modelo las procese.</li>
                <li>Estos vectores capturan no solo la identidad de cada token, sino tambi√©n sus <span class="highlight">caracter√≠sticas sem√°nticas y sint√°cticas</span>, permitiendo al modelo entender relaciones y contextos.</li>
            </ul>
            <p>
                üìå **Analogy:** Imagina que representas "banana" y "mango" en un espacio bidimensional:
                <br>Banana ‚Üí (2,2)
                <br>Mango ‚Üí (13,2)
                <br>Ambas comparten la categor√≠a "fruta", lo cual se refleja en su proximidad en ciertas dimensiones del espacio vectorial. En la pr√°ctica, los embeddings existen en un espacio n-dimensional, donde la distancia y direcci√≥n entre vectores denotan similitudes y diferencias de significado.
            </p>

            <h3><span class="emoji">3.</span> Codificaci√≥n Posicional (Positional Encoding)</h3>
            <p>
                Dado que la arquitectura Transformer procesa las palabras de una secuencia en paralelo y no de forma secuencial como las RNN, necesita un mecanismo para inyectar informaci√≥n sobre la <span class="highlight">posici√≥n relativa y absoluta</span> de cada token.
            </p>
            <p>
                Para lograr esto, se a√±aden a los embeddings de entrada unas codificaciones posicionales √∫nicas, generadas utilizando funciones seno y coseno. Estas funciones permiten que el modelo inferir la posici√≥n de cada token y, por ende, su orden dentro de la secuencia.
            </p>
            <div class="equation">
                $PE(pos,2i) = sin(pos / 10000^{2i / d_{model}})$ <br>
                $PE(pos,2i+1) = cos(pos / 10000^{2i / d_{model}})$
            </div>
            <p>
                Este ingenioso m√©todo permite al modelo mantener la noci√≥n del orden de las palabras, crucial para la comprensi√≥n del lenguaje.
            </p>

            <h3><span class="emoji">4.</span> Auto-atenci√≥n (Self-Attention)</h3>
            <p>
                Este es el <span class="highlight">mecanismo fundamental</span> del Transformer. Permite al modelo identificar relaciones entre palabras sin importar su distancia en la secuencia. Por ejemplo, en la oraci√≥n "Paul fue a la tienda y luego <span class="highlight">√©l</span> compr√≥ pan", la auto-atenci√≥n ayuda al modelo a entender que "√©l" se refiere a "Paul".
            </p>
            <p>
                Se basa en una funci√≥n que calcula la similitud entre una consulta (Query, $Q$), un conjunto de claves (Keys, $K$) y sus valores asociados (Values, $V$). Intuitivamente, busca determinar qu√© partes de la entrada son m√°s relevantes para la palabra actual que se est√° procesando.
            </p>

            <h3><span class="emoji">5.</span> Atenci√≥n Escalada de Producto Punto (Scaled Dot-Product Attention)</h3>
            <p>
                Esta es la forma espec√≠fica en que se implementa la auto-atenci√≥n. Se comparan las palabras entre s√≠ mediante el producto escalar de vectores $Q$ (Query), $K$ (Key) y $V$ (Value), generando pesos que indican la relevancia entre ellas.
            </p>
            <div class="equation">
                $Attention(Q, K, V) = softmax(QK^T / \sqrt{d_k}) \cdot V$
            </div>
            <p>
                Donde $d_k$ es la dimensi√≥n de las claves. El resultado de esta operaci√≥n es una <span class="highlight">suma ponderada de los valores</span>, donde los pesos son determinados por las puntuaciones de atenci√≥n calculadas. Esto permite al modelo enfocarse din√°micamente en la informaci√≥n m√°s relevante.
            </p>

            <h3><span class="emoji">6.</span> Atenci√≥n Multi-Cabeza (Multi-Head Attention)</h3>
            <p>
                Para enriquecer a√∫n m√°s la comprensi√≥n contextual, el Transformer no utiliza una √∫nica capa de atenci√≥n, sino que ejecuta <span class="highlight">m√∫ltiples mecanismos de atenci√≥n en paralelo</span> (por ejemplo, 8 "cabezas").
            </p>
            <p>
                Cada "cabeza" de atenci√≥n aprende a enfocarse en diferentes relaciones y patrones dentro de la secuencia. Por ejemplo, una cabeza podr√≠a capturar dependencias sint√°cticas (sujeto-verbo), mientras que otra podr√≠a identificar relaciones sem√°nticas (sin√≥nimos, ant√≥nimos). Los resultados de todas las cabezas se concatenan y se proyectan linealmente para formar la salida final. Esto permite al modelo analizar el contexto desde <span class="highlight">m√∫ltiples √°ngulos simult√°neamente</span>, sin a√±adir tiempo de c√≥mputo significativo.
            </p>

            <h3><span class="emoji">üßÆ</span> Red Feed-Forward Posicional (Position-wise Feed-Forward Network)</h3>
            <p>
                Tras el mecanismo de atenci√≥n, cada posici√≥n en la secuencia pasa por una red neuronal simple y completamente conectada. Es importante destacar que esta red se aplica <span class="highlight">de forma independiente y id√©ntica a cada posici√≥n</span>.
            </p>
            <div class="equation">
                $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$
            </div>
            <p>
                Esta red permite al modelo introducir no linealidades y procesar a√∫n m√°s las representaciones generadas por el mecanismo de atenci√≥n.
            </p>
            <p>
                Adicionalmente, cada bloque del Transformer incorpora:
            </p>
            <ul>
                <li><span class="emoji">üîÑ</span> **Conexiones Residuales:** Ayudan a que el gradiente fluya m√°s f√°cilmente a trav√©s de capas profundas, mitigando el problema de desvanecimiento del gradiente y facilitando el entrenamiento.</li>
                <li><span class="emoji">üìè</span> **Normalizaci√≥n de Capas:** Normaliza las activaciones dentro de cada capa para estabilizar el proceso de entrenamiento y acelerar la convergencia.</li>
            </ul>

            <h3><span class="emoji">üìä</span> Capa Lineal y Softmax Final</h3>
            <p>
                Al finalizar el procesamiento en el decoder, el modelo necesita transformar sus representaciones internas en predicciones tangibles, como la probabilidad de cada palabra en el vocabulario de salida.
            </p>
            <ul>
                <li>**Capa Lineal:** Una capa densa convierte los vectores de salida del decoder en <span class="highlight">puntuaciones (logits)</span> para cada palabra potencial en el vocabulario.</li>
                <li>**Funci√≥n Softmax:** Esta funci√≥n de activaci√≥n convierte los logits en una <span class="highlight">distribuci√≥n de probabilidades</span> sobre todo el vocabulario, donde la suma de todas las probabilidades es 1.</li>
            </ul>
            <p>
                ‚ú® **Ejemplo:** Si la entrada es "El gato est√°‚Ä¶", la salida probable podr√≠a ser:
                <br>‚Ä¢ dormido (85%)
                <br>‚Ä¢ corriendo (10%)
                <br>‚Ä¢ feliz (5%)
                <br>El modelo seleccionar√≠a la palabra con la probabilidad m√°s alta ("dormido" en este caso) como la siguiente en la secuencia.
            </p>
        </section>

        <section id="flujo">
            <h2><span class="emoji">üîÑ</span> Flujo del Modelo Transformer</h2>
            <p>
            
                    <img src="image-1024x760.webp" alt="Diagrama de Arquitectura Transformer">
                El proceso de c√≥mo el Transformer maneja las secuencias se puede dividir en dos fases principales:  
            <h3><span class="emoji">üì•</span> Codificaci√≥n (Proceso del Encoder)</h3>
            <p>
                La fase de codificaci√≥n toma la secuencia de entrada y la prepara para ser comprendida por el modelo:
            </p>
            <ul>
                <li>**Embedding + Posici√≥n:** Cada palabra de la secuencia de entrada se convierte en un vector (embedding) y se le a√±ade la codificaci√≥n posicional para mantener la informaci√≥n de orden.</li>
                <li>**Auto-atenci√≥n M√∫ltiple:** Las representaciones de las palabras pasan a trav√©s de m√∫ltiples capas de auto-atenci√≥n multi-cabeza, permitiendo que cada palabra "atienda" a otras palabras en la misma secuencia para construir un contexto rico.</li>
                <li>**Red Neuronal Feed Forward:** Despu√©s de la auto-atenci√≥n, cada posici√≥n pasa por una red neuronal feed-forward independiente para procesar a√∫n m√°s la informaci√≥n contextualizada.</li>
            </ul>

            <h3><span class="emoji">üì§</span> Decodificaci√≥n (Proceso del Decoder)</h3>
            <p>
                La fase de decodificaci√≥n genera la secuencia de salida palabra por palabra, bas√°ndose en la informaci√≥n del encoder y las palabras ya generadas:
            </p>
            <ul>
                <li>**Embedding de la palabra anterior:** La palabra generada en el paso anterior se convierte en un embedding y se le a√±ade su codificaci√≥n posicional.</li>
                <li>**Posici√≥n + Auto-atenci√≥n en el Output:** Esta representaci√≥n pasa por una capa de auto-atenci√≥n enmascarada, lo que significa que una palabra solo puede atender a las palabras que la preceden en la secuencia de salida (para evitar "hacer trampa" y ver el futuro).</li>
                <li>**Atenci√≥n Cruzada con la salida del Encoder:** Una capa de atenci√≥n adicional permite que el decoder "atienda" a las representaciones clave generadas por el encoder. Esto es crucial para que el decoder sepa qu√© partes de la entrada son relevantes para generar la siguiente palabra de salida.</li>
                <li>**Feed Forward + Capa Final + Softmax:** Finalmente, la salida pasa por otra red neuronal feed-forward y una capa lineal seguida de una funci√≥n Softmax para producir la distribuci√≥n de probabilidad de la siguiente palabra en el vocabulario.</li>
            </ul>
        </section>

        <section id="ventajas">
            <h2><span class="emoji">‚öôÔ∏è</span> Ventajas Sobresalientes de la Arquitectura Transformer</h2>
            <p>
                El Transformer ha superado a las arquitecturas previas de PLN en varios aspectos cr√≠ticos. La siguiente tabla resume sus principales ventajas:
            </p>
            <table>
                <thead>
                    <tr>
                        <th><span class="emoji">üß™</span> M√©trica</th>
                        <th>RNN (Redes Recurrentes)</th>
                        <th>CNN (Redes Convolucionales)</th>
                        <th>Transformer</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Paralelizaci√≥n</td>
                        <td>‚ùå</td>
                        <td>‚úÖ</td>
                        <td>‚úÖ‚úÖ‚úÖ</td>
                    </tr>
                    <tr>
                        <td>Eficiencia en secuencias largas</td>
                        <td>‚ùå</td>
                        <td>‚úÖ</td>
                        <td>‚úÖ‚úÖ‚úÖ</td>
                    </tr>
                    <tr>
                        <td>Interpretabilidad (atenci√≥n)</td>
                        <td>‚ùå</td>
                        <td>‚ùå</td>
                        <td>‚úÖ‚úÖ</td>
                    </tr>
                    <tr>
                        <td>Complejidad computacional</td>
                        <td>$O(n \cdot d^2)$</td>
                        <td>$O(k \cdot n \cdot d^2)$</td>
                        <td>$O(n^2 \cdot d)$</td>
                    </tr>
                </tbody>
            </table>
            <p>
                üü¢ El Transformer reduce la longitud del camino para capturar dependencias a $O(1)$, frente a $O(n)$ en RNN.
            </p>
            <h3><span class="emoji">üöÄ</span> ¬øPor qu√© es tan potente?</h3>
            <ul>
                <li>Mejora significativamente el rendimiento frente a arquitecturas tradicionales como RNN, LSTM y GRU.</li>
                <li>Procesa textos m√°s largos sin perder contexto, gracias a su mecanismo de atenci√≥n que puede conectar palabras distantes.</li>
                <li>Entrena m√°s r√°pido debido a su capacidad de paralelizaci√≥n, a diferencia de las RNN que procesan secuencialmente.</li>
                <li>Es la base de modelos modernos y de vanguardia como BERT, GPT, RoBERTa, XLNet, y muchos otros que han impulsado el estado del arte en NLP.</li>
            </ul>
        </section>

        <section id="resultados">
            <h2><span class="emoji">üìà</span> Resultados y Rendimiento</h2>
            <p>
                La introducci√≥n del Transformer marc√≥ hitos significativos en el rendimiento de diversas tareas de PLN:
            </p>
            <ul>
                <li>WMT 2014 Ingl√©s ‚Üí Alem√°n: Logr√≥ un impresionante 28.4 BLEU, estableciendo un nuevo r√©cord en su √©poca.</li>
                <li>WMT 2014 Ingl√©s ‚Üí Franc√©s: Alcanz√≥ 41.0 BLEU con tan solo 3.5 d√≠as de entrenamiento utilizando 8 GPUs.</li>
                <li>Super√≥ consistentemente a modelos previos de vanguardia como GNMT, ConvS2S y ByteNet, tanto en velocidad de entrenamiento como en calidad de los resultados obtenidos.</li>
            </ul>
        </section>

        <section id="aplicaciones">
            <h2><span class="emoji">üì¶</span> Aplicaciones del Transformer</h2>
            <p>
                La versatilidad y el rendimiento superior del Transformer lo han convertido en la arquitectura base para una amplia gama de aplicaciones de IA, incluyendo:
            </p>
            <ul>
                <li>üìö **Traducci√≥n autom√°tica:** Es el pilar de sistemas modernos como Google Translate, permitiendo traducciones m√°s fluidas y precisas.</li>
                <li>ü§ñ **Modelos de lenguaje:** La base de modelos generativos y de comprensi√≥n de texto como GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), T5, etc.</li>
                <li>üéß **Reconocimiento y an√°lisis de audio:** Utilizado en tareas de transcripci√≥n de voz a texto y an√°lisis de patrones en se√±ales de audio.</li>
                <li>üß¨ **Predicci√≥n de estructuras biol√≥gicas:** Aplicaciones innovadoras en bioinform√°tica, como el revolucionario AlphaFold de DeepMind para la predicci√≥n de estructuras de prote√≠nas.</li>
                <li>üñºÔ∏è **Visi√≥n por computadora:** Adaptaciones como Vision Transformer (ViT) han demostrado un rendimiento excepcional en tareas de clasificaci√≥n de im√°genes y detecci√≥n de objetos.</li>
            </ul>
        </section>

        <section id="implementacion">
            <h2><span class="emoji">üõ†Ô∏è</span> Implementaci√≥n en C√≥digo</h2>
            <p>
                Para aquellos interesados en construir o experimentar con modelos Transformer, existen diversos frameworks y librer√≠as que facilitan su implementaci√≥n:
            </p>
            <ul>
                <li>**Frameworks de Deep Learning:**
                    <ul>
                        <li>TensorFlow</li>
                        <li>PyTorch</li>
                        <li>JAX</li>
                    </ul>
                </li>
                <li>**Librer√≠as especializadas:**
                    <ul>
                        <li><span class="highlight">HuggingFace Transformers:</span> Una de las librer√≠as m√°s populares y completas para trabajar con modelos basados en Transformer.</li>
                        <li>Tensor2Tensor: Librer√≠a original de Google Brain que implementa el Transformer.</li>
                        <li>fairseq: Un toolkit de Facebook AI Research para modelos de secuencia a secuencia.</li>
                    </ul>
                </li>
            </ul>
            <p>
                Puedes encontrar el c√≥digo oficial de los autores en GitHub:
                <br><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener noreferrer">
                    üîó C√≥digo oficial de los autores en GitHub ‚Äì Tensor2Tensor
                </a>
            </p>
        </section>

        <section id="video">
            <h2><span class="emoji">üé•</span> Video Explicativo</h2>
            <p>
                Para una comprensi√≥n m√°s visual y din√°mica de la arquitectura Transformer, te invitamos a ver el siguiente video:
            </p>
            <div class="video-container">
                <iframe
                    src="https://www.youtube.com/embed/o-pf_pdbDgo?si=kBIN7f_D010GU8W0"
                    title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin"
                    allowfullscreen
                ></iframe>
            </div>
        </section>

        <section id="conclusion">
            <h2><span class="emoji">üìù</span> Conclusi√≥n</h2>
            <p>
                El Transformer no es solo una arquitectura m√°s; ha <span class="highlight">transformado por completo</span> el panorama del procesamiento de lenguaje y otras tareas secuenciales en la inteligencia artificial.
            </p>
            <p>
                Gracias a su estructura innovadora, basada completamente en el mecanismo de atenci√≥n, ha demostrado la capacidad de entrenarse m√°s r√°pido, manejar contextos de dependencia m√°s largos y lograr resultados superiores en una multitud de benchmarks.
            </p>
            <p>
                Hoy en d√≠a, el Transformer es la piedra angular sobre la que se construyen los modelos de inteligencia artificial m√°s avanzados y potentes del mundo, desde asistentes virtuales hasta sistemas de generaci√≥n de contenido y traducci√≥n. Su impacto contin√∫a expandi√©ndose a nuevos dominios, consolidando su posici√≥n como una de las innovaciones m√°s importantes en el campo del aprendizaje profundo.
            </p>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Arquitectura Transformer. Todos los derechos reservados.</p>
    </footer>

</body>
</html>
